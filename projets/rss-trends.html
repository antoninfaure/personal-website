<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <title>Antonin Faure</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="author" content="Antonin Faure">

    <!-- JQuery -->
    <script src="https://code.jquery.com/jquery-3.4.1.min.js"></script>

    <!-- Bootstrap -->
    <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>

    <!-- Custom -->
    <link type="text/css" rel="stylesheet" href="../static/assets/style.css"/>

    <!-- D3.js -->
    <script src="https://d3js.org/d3.v4.min.js"></script>

    <link href="https://unpkg.com/prismjs@v1.x/themes/prism-tomorrow.min.css" rel="stylesheet" />
    <script src="https://unpkg.com/prismjs@v1.x/components/prism-core.min.js"></script>
	<script src="https://unpkg.com/prismjs@v1.x/plugins/autoloader/prism-autoloader.min.js"></script>
</head>
<body>
    <div class="row m-0 p-0 fill">
        <div class="aside shadow">
            <a href="../index.html" class="btn btn-light m-3">Accueil</a>
            <div class="col-12 p-3">
                <h3>RSS Trends</h3>
                <p class="f-gilroy-l font-weight-bold" style="font-size: 1.3rem;">Objectif : faire une analyse des flux RSS des grands médias français pour créer un Text Network représentant les tendances de l'actualité ainsi que les liens entre les termes les plus fréquents.</p>
                <a href="https://github.com/antoninfaure/rssTrends" class="btn btn-dark mt-3">GitHub</a>
            </div>
            <div class="col-12 p-3">
                <h5>Made with</h5>
                <button class="btn btn-danger my-2">Jupyter Notebook</button>
                <a href="https://gephi.org/" class="btn btn-light my-2">Gephi</a>
            </div>
        </div>
        <div class="container-right">
            <!--<svg id="mynetwork"></svg>-->
            <iframe src="https://antoninfaure.github.io/rssTrends/" frameBorder="0" id="iframeNetwork"></iframe>
            
            <div class="row m-0 p-0">
                <div class="col-12 m-0 p-5">
                    <h3>Data Mining</h3>
                    <p>Pour récupérer des articles d'actualités françaises je me suis basé sur les flux RSS des médias suivants : </p>
                    <div class="box">
                        <pre>
                            <code class="language-python">
feed_urls = [
    "http://www.lemonde.fr/rss/une.xml",
    "https://www.bfmtv.com/rss/news-24-7/",
    "https://www.liberation.fr/rss/",
    "http://www.lefigaro.fr/rss/figaro_actualites.xml",
    "https://www.franceinter.fr/rss",
    "https://www.lexpress.fr/arc/outboundfeeds/rss/alaune.xml",
    "https://www.francetvinfo.fr/titres.rss",
    "https://www.la-croix.com/RSS",
    "http://tempsreel.nouvelobs.com/rss.xml",
    "http://www.lepoint.fr/rss.xml",
    "https://www.france24.com/fr/rss",
    "https://feeds.leparisien.fr/leparisien/rss",
    "https://www.ouest-france.fr/rss/une",
    "https://www.europe1.fr/rss.xml",
    "https://partner-feeds.20min.ch/rss/20minutes",
    "https://www.afp.com/fr/actus/afp_actualite/792,31,9,7,33/feed"
]
                            </code>
                        </pre>

                    </div>
                    <p>Un rapide script pour récupérer les titres et descriptions de tous les articles avec l'utilisation des librairies BeautifulSoup, Pandas et requests.</p>
                    <div class="box">
                        <pre>
                            <code class="language-python">
def scrap(feed_urls):
    news_list = pd.DataFrame(columns=('title', 'summary'))

    for feed_url in feed_urls:
        res = requests.get(feed_url)
        feed = BeautifulSoup(res.content, features='xml')

        articles = feed.findAll('item')       
        for article in articles:
            title = BeautifulSoup(article.find('title').get_text(), "html").get_text()
            summary = ""
            if (article.find('description')):
                summary = BeautifulSoup(article.find('description').get_text(), "html").get_text()
                news_list.loc[len(news_list)] = [title, summary]

    return news_list
                            </code>
                        </pre>
                    </div>
                    <p class="mt-3">Il faut ensuite traiter le texte des articles à l'aide des librairies Spacy et NLTK qui parsent le texte en enlevant les charactères spéciaux, puis qui tokenize chaques termes et enfin qui les lemmatisent. On calcule aussi le vocabulaire ainsi que ses fréquences selon le corpus.</p>
                    <div class="box">
                        <pre>
                            <code class="language-python">
def process_text(docs, lang='fr'):
    if (lang=='fr'):
        nlp = spacy.load('fr_core_news_sm')
    elif (lang=='en'):
        nlp = spacy.load('en_core_web_sm')

    # Utility functions
    punctuation_chars =  [
        chr(i) for i in range(sys.maxunicode)
        if category(chr(i)).startswith("P")
    ]
    def tokenize(text):
        text = "".join(list(filter(lambda x: x not in [*string.punctuation, *punctuation_chars], text)))
        tokens = nltk.word_tokenize(text)
        words = list(filter(lambda x: x not in [stopwords.words('english') + stopwords.words('french')], tokens))
        return list(map(lambda x: x.lower(), words))

    def preprocess_text(documents):
        docs = list(map(lambda doc: tokenize(doc), documents))
        return docs

    # Clean and tokenize docs
    tokenized_docs = preprocess_text(docs)

    # Lemmanize docs
    def lemmanize(doc):
        doc = list(filter(lambda token: token.lemma_ not in nlp.Defaults.stop_words, doc))
        return list(map(lambda token: token.lemma_, doc))

    lemma_docs = list(map(lambda doc: lemmanize(nlp(" ".join(doc))), tokenized_docs))

    def get_vocabulary_frequency(documents):
        vocabulary = dict()
        for doc in documents:
            for word in doc:
                print(word)
                if word in list(vocabulary.keys()):
                    vocabulary[word] += 1
                else:
                    vocabulary[word] = 1

        return vocabulary

    voc = get_vocabulary_frequency(lemma_docs)

    return lemma_docs, voc
                            </code>
                        </pre>
                    </div>
                    <h3>Data Visualization</h3>
                    <p>Afin de visualiser le network, il faut dans un premier temps lister les liens (edges) entre chaques termes (nodes). Pour ce faire, on utilise la librairie NLTK et sa méthode pour calculer les bigrammes (i.e. les paires de termes voisins dans une phrase). Chaque bigramme représente donc un <strong>lien</strong> tandis que chaque terme représente un <strong>noeud</strong> dont la taille dépend de sa fréquence dans le corpus.</p>
                    <div class="box">
                        <pre>
                            <code class="language-python">
def graphnet(docs, voc, min_freq=5):
    # Filter voc with min_freq
    filtered_voc = dict(filter(lambda elem: elem[1] > min_freq, voc.items()))

    term_to_voc_id = dict()
    for i, term in enumerate(filtered_voc):
        term_to_voc_id[term] = i

    # List bigrams (edges)
    finder = nltk.BigramCollocationFinder.from_documents(docs)
    bigram_measures = nltk.collocations.BigramAssocMeasures()
    bigrams = list(finder.score_ngrams(bigram_measures.raw_freq))
    bigrams = list(map(lambda x: x[0], bigrams))

    # Filter the bigrams with filtered_voc elements and replace by terms by their id
    bigrams = list(filter(lambda x: x[0] in filtered_voc.keys() and x[1] in filtered_voc.keys(), bigrams))
    bigrams = list(map(lambda x: (term_to_voc_id[x[0]], term_to_voc_id[x[1]]), bigrams))

    # Set nodes sizes
    sizes = list(filtered_voc.values())

    # Format data
    nodes = []
    for i, term in enumerate(filtered_voc.keys()):
        nodes.append({
            'id': i,
            'label': term,
            'size': sizes[i]
        })
    
    edges = []
    for i, edge in enumerate(bigrams):
        (source, target) = edge
        edges.append({
            'id': i,
            'source': source,
            'target': target
        })

    
    # Write JSON files
    with open('nodes.json', 'w', encoding='UTF8', newline='') as f:
        writer = json.dump(nodes, f, ensure_ascii=False)

    
    with open('edges.json', 'w', encoding='UTF8', newline='') as f:
        writer = json.dump(edges, f, ensure_ascii=False)
                            </code>
                        </pre>
                    </div>
                    <p>On peut ensuite afficher le network à l'aide de la libraire D3.js, comme visible en haut de la page. On peut aussi utiliser le logiciel <a href="https://gephi.org/">Gephi</a> permettant la manipulation de large set de données, inenvisageable autrement pour le set des articles US 2022 (~250,000 articles). Ci-dessous le résultat sur Gephi pour les actualités françaises du 30 janvier 2023.</p>
                    <img src="../static/images/projets/rss-trends/rss-trends.png" class="img-fluid mt-2">
                </div>
            </div>
        </div>
    </div>
</body>
<script type="module" src="../static/assets/rss-trends/rss-trends.js"></script>
<style>
    html, body {
        min-height: 100%;
        height: 100%;
        min-width: 100%;
        margin: 0;
        padding: 0;
    }
    .fill { 
        min-height: 100%;
        height: 100%;
    }
    a, a:hover {
        color: inherit;
        text-decoration: inherit;
    }
    .aside {
        position: fixed;
        width: 500px;
        height: 100%;
        padding: 30px;
        margin: 0;
        background-color:#F9BE08;
        color: black;
    }
    .container-right {
        margin-left: 500px;
        padding: 0;
    }
    .text-label {
        font-family: 'GilroyBold';
    }
    #iframeNetwork {
      width: 100%;
      min-height: 600px;
      background-color: white;
      height: 80vh;
    }
    .title {
        font-family: 'GilroyBold';
        font-size: 1.5rem;
    }
    .language-python {
        background: none !important;
        padding: 0 !important;
    }
    p {
        margin: 0;
        font-size: 1.2rem;
        text-align: justify;
    }
    @media screen and (max-width: 980px) {
        .aside {
            position: relative;
            height: auto;
            width: 100%;
        }
        .container-right {
            margin: 0;
            width: 100%;
        }
    }
    .ticks {
  font: 12px sans-serif;
  fill: black;
}

.track,
.track-inset,
.track-overlay {
  stroke-linecap: round;
}

.track {
  stroke: #000;
  stroke-opacity: 0.3;
  stroke-width: 10px;
}

.track-inset {
  stroke: #ddd;
  stroke-width: 8px;
}

.track-overlay {
  pointer-events: stroke;
  stroke-width: 50px;
  cursor: pointer;
}

.handle {
  fill: #fff;
  stroke: #000;
  stroke-opacity: 0.5;
  stroke-width: 1.25px;
}
</style>
</html>