<!DOCTYPE html>
<html lang="fr">

<head>
    <meta charset="UTF-8">
    <title>Antonin Faure</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="author" content="Antonin Faure">

    <!-- JQuery -->
    <script src="https://code.jquery.com/jquery-3.4.1.min.js"></script>

    <!-- Bootstrap -->
    <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js"
        integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6"
        crossorigin="anonymous"></script>

    <!-- Custom -->
    <link type="text/css" rel="stylesheet" href="../static/assets/style.css" />

    <!-- D3.js -->
    <script src="https://d3js.org/d3.v4.min.js"></script>

    <link href="https://unpkg.com/prismjs@v1.x/themes/prism-tomorrow.min.css" rel="stylesheet" />
    <script src="https://unpkg.com/prismjs@v1.x/components/prism-core.min.js"></script>
    <script src="https://unpkg.com/prismjs@v1.x/plugins/autoloader/prism-autoloader.min.js"></script>

    <link rel="stylesheet" type="text/css" href="https://cdn.datatables.net/1.13.2/css/jquery.dataTables.css">

    <script type="text/javascript" charset="utf8"
        src="https://cdn.datatables.net/1.13.2/js/jquery.dataTables.js"></script>
</head>

<body>
    <div class="row m-0 p-0 fill">
        <div class="aside shadow-lg">
            <a href="../index.html" class="btn btn-light m-3">Accueil</a>
            <div class="col-12 p-3">
                <h3>RSS Trends</h3>
                <p class="f-gilroy-l font-weight-bold" style="font-size: 1.3rem;">Objectif : faire une analyse des flux
                    RSS des grands médias français pour créer un Text Network représentant les tendances de l'actualité
                    ainsi que les liens entre les termes les plus fréquents.</p>
                <a href="https://github.com/antoninfaure/rssTrends" class="btn btn-dark mt-3">GitHub</a>
            </div>
            <div class="col-12 p-3">
                <h5>Made with</h5>
                <button class="btn btn-danger my-2">Jupyter Notebook</button>
                <a href="https://gephi.org/" class="btn btn-light my-2">Gephi</a>
            </div>
        </div>
        <div class="container-right">
            <iframe src="https://antoninfaure.github.io/rssTrends/" frameBorder="0" id="iframeNetwork"></iframe>
            <div class="row m-0 p-0">
                <div class="col-12 m-0 p-0">
                    <div class="col-12 m-0 p-5">


                        <h3>Data Mining</h3>
                        <p>Pour récupérer des articles d'actualités françaises je me suis basé sur les flux RSS des
                            médias
                            suivants : </p>
                        <div class="box">
                            <pre>
                            <code class="language-python">
feed_urls = [
    "http://www.lemonde.fr/rss/une.xml",
    "https://www.bfmtv.com/rss/news-24-7/",
    "https://www.liberation.fr/rss/",
    "http://www.lefigaro.fr/rss/figaro_actualites.xml",
    "https://www.franceinter.fr/rss",
    "https://www.lexpress.fr/arc/outboundfeeds/rss/alaune.xml",
    "https://www.francetvinfo.fr/titres.rss",
    "https://www.la-croix.com/RSS",
    "http://tempsreel.nouvelobs.com/rss.xml",
    "http://www.lepoint.fr/rss.xml",
    "https://www.france24.com/fr/rss",
    "https://feeds.leparisien.fr/leparisien/rss",
    "https://www.ouest-france.fr/rss/une",
    "https://www.europe1.fr/rss.xml",
    "https://partner-feeds.20min.ch/rss/20minutes",
    "https://www.afp.com/fr/actus/afp_actualite/792,31,9,7,33/feed"
]
                            </code>
                        </pre>

                        </div>
                        <p>Un rapide script pour récupérer les titres et descriptions de tous les articles avec
                            l'utilisation des librairies BeautifulSoup, Pandas et requests.</p>
                        <div class="box">
                            <pre>
                            <code class="language-python">
def scrap(feed_urls):
    news_list = pd.DataFrame(columns=('title', 'summary'))

    for feed_url in feed_urls:
        res = requests.get(feed_url)
        feed = BeautifulSoup(res.content, features='xml')

        articles = feed.findAll('item')       
        for article in articles:
            title = BeautifulSoup(article.find('title').get_text(), "html").get_text()
            summary = ""
            if (article.find('description')):
                summary = BeautifulSoup(article.find('description').get_text(), "html").get_text()
                news_list.loc[len(news_list)] = [title, summary]

    return news_list
                            </code>
                        </pre>
                        </div>
                        <p>Il faut ensuite traiter le texte des articles à l'aide des librairies Spacy et NLTK
                            qui parsent le texte en enlevant les charactères spéciaux, puis qui tokenize chaque terme
                            et
                            enfin qui les lemmatisent. On calcule aussi le vocabulaire ainsi que ses fréquences selon le
                            corpus.</p>
                        <div class="box">
                            <pre>
                            <code class="language-python">
def process_text(docs, lang='fr'):
    if (lang=='fr'):
        nlp = spacy.load('fr_core_news_lg')
    elif (lang=='en'):
        nlp = spacy.load('en_core_web_sm')

    # Utility functions
    punctuation_chars =  [
        chr(i) for i in range(sys.maxunicode)
        if category(chr(i)).startswith("P")
    ]

    lemma_docs = []
    for doc in docs:
        # Tokenize doc
        tokenized_doc = nlp(doc)

        # Lemmanize doc
        lemma_doc = list(filter(lambda token: token.is_stop == False and token.pos_ in ['NOUN', 'PROPN'] and token.lemma_ not in [*string.punctuation, *punctuation_chars], tokenized_doc))
        lemma_doc = list(map(lambda tok: tok.lemma_, lemma_doc))
        lemma_docs.append(lemma_doc)


    def get_vocabulary_frequency(documents):
        vocabulary = dict()
        for doc in documents:
            for word in doc:
                if word in list(vocabulary.keys()):
                    vocabulary[word] += 1
                else:
                    vocabulary[word] = 1

        return vocabulary

    voc = get_vocabulary_frequency(lemma_docs)

    return lemma_docs, voc
                            </code>
                        </pre>
                        </div>
                    </div>
                    <div class="wave-container">
                        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1440 200">
                            <path fill="#FFF" fill-opacity="1"
                                d="M0,64L120,80C240,96,480,128,720,138.7C960,149,1200,139,1320,133.3L1440,128L1440,320L1320,320C1200,320,960,320,720,320C480,320,240,320,120,320L0,320Z">
                            </path>
                        </svg>
                    </div>

                </div>
                <div class="col-12 m-0 p-0 light">
                    <div class="col-12 m-0 p-5">
                        <h3>Data Visualization</h3>
                        <p>Afin de visualiser le network, il faut dans un premier temps lister les liens (edges) entre
                            chaque terme (nodes). Pour ce faire, on utilise la librairie NLTK et sa méthode pour
                            calculer
                            les bigrammes (i.e. les paires de termes voisins dans une phrase). Chaque bigramme
                            représente
                            donc un <strong>lien</strong> tandis que chaque terme représente un <strong>noeud</strong>
                            dont
                            la taille dépend de sa fréquence dans le corpus.</p>
                        <div class="box">
                            <pre>
                            <code class="language-python">
def graphnet(docs, voc, min_freq=5):

    # Filter voc with min_freq
    filtered_voc = dict(filter(lambda elem: elem[1] > min_freq, voc.items()))

    dict_voc_id = dict()
    for i, term in enumerate(filtered_voc):
        dict_voc_id[term] = i

    # List bigrams (edges)
    finder = nltk.BigramCollocationFinder.from_documents(docs)
    bigram_measures = nltk.collocations.BigramAssocMeasures()
    bigrams = list(finder.score_ngrams(bigram_measures.raw_freq))
    min_freq = min(list(map(lambda x: x[1], bigrams)))
    bigrams = list(map(lambda x: (x[0], x[1]/min_freq), bigrams))

    # Filter the bigrams with filtered_voc elements and replace by id
    filtered_bigrams = []
    for bigram in bigrams:
        if (bigram[0][0] in filtered_voc.keys() and bigram[0][1] in filtered_voc.keys()):
            #new_bigram = ( dict_voc_id[bigram[0][0]] , dict_voc_id[bigram[0][1]] )
            new_bigram = bigram[0]
            filtered_bigrams.append((new_bigram, bigram[1]))

    # Set nodes sizes
    sizes = list(filtered_voc.values())

    # Format data
    nodes = []
    for i, term in enumerate(filtered_voc.keys()):
        nodes.append({
            'id': term,
            'label': term,
            'size': sizes[i]
        })

    edges = []
    for i, edge in enumerate(filtered_bigrams):
        (source, target) = edge[0]
        edges.append({
            'id': i,
            'source': source,
            'target': target,
            'size': edge[1]
        })


    # Write JSON files
    output_file(nodes, 'nodes.json')

    output_file(edges, 'edges.json')
                            </code>
                        </pre>
                        </div>
                        <p>On peut ensuite afficher le network à l'aide de la libraire D3.js, comme visible en haut de
                            la
                            page. On peut aussi utiliser le logiciel <a href="https://gephi.org/">Gephi</a> permettant
                            la
                            manipulation de large set de données, inenvisageable autrement pour le set des articles US
                            2022
                            (~250,000 articles). Ci-dessous le résultat sur Gephi pour les actualités françaises du 30
                            janvier 2023.</p>
                        <img src="../static/images/projets/rss-trends/rss-trends.png" class="img-fluid mt-2">

                    </div>
                </div>
                <div class="col-12 m-0 p-0">
                    <div class="wave-container">
                        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 50 1440 220">
                            <path fill="#FFF" fill-opacity="1"
                                d="M0,64L120,58.7C240,53,480,43,720,69.3C960,96,1200,160,1320,192L1440,224L1440,0L1320,0C1200,0,960,0,720,0C480,0,240,0,120,0L0,0Z">
                            </path>
                        </svg>
                    </div>
                    <div class="col-12 m-0 px-5">
                        <h3>Association Rules</h3>
                        <p>Afin d'obtenir les sujets les plus tendances on peut se baser sur différents critères
                            d'association rules : confidence, support, lift, added value, leverage, conviction. Dans un
                            premier temps on crée une tdf-matrix (term-document frequency) afin de créer les différentes
                            k-combinaisons de termes.</p>
                        <div class="box">
                            <pre>
                            <code class="language-python">
    te = TransactionEncoder()
    te_ary = te.fit(docs).transform(docs, sparse=True)
    df = pd.DataFrame.sparse.from_spmatrix(te_ary, columns=te.columns_)
                            </code>
                        </pre>
                        </div>
                        <p>On applique ensuite l'apriori algorithm pour obtenir les k-combinaisons (avec k>1) les plus
                            pertinentes.</p>
                        <div class="box">
                            <pre>
                            <code class="language-python">
    frequent_itemsets = apriori(df, min_support=0.005, use_colnames=True, verbose=1)
    frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))

    rules = association_rules(frequent_itemsets, metric ="lift", min_threshold = 1)
    rules = rules.sort_values([criterion], ascending =[False])

    rules = rules[rules[criterion] > level]
                            </code>
                        </pre>
                        </div>
                        <p>Néanmoins il s'avère que plusieurs combinaisons peuvent représenter le même "topic" et il
                            serait
                            donc pertinent de fusionner les combinaisons afin d'obtenir le condensé du "topic".
                            <br>Ci-dessous un extrait des combinaisons les plus pertinentes pour les données du 13
                            février 2023 :</p>
                        <div class="box" style="overflow-x: scroll;">
                            <table class="dataframe">
                                <thead>
                                    <tr style="text-align: right;">
                                        <th></th>
                                        <th>antecedents</th>
                                        <th>consequents</th>
                                        <th>support</th>
                                        <th>confidence</th>
                                        <th>lift</th>
                                        <th>leverage</th>
                                        <th>conviction</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <th>141</th>
                                        <td>(Ukraine)</td>
                                        <td>(guerre)</td>
                                        <td>0.048507</td>
                                        <td>0.812500</td>
                                        <td>14.048387</td>
                                        <td>0.045055</td>
                                        <td>5.024876</td>
                                    </tr>
                                    <tr>
                                        <th>140</th>
                                        <td>(guerre)</td>
                                        <td>(Ukraine)</td>
                                        <td>0.048507</td>
                                        <td>0.838710</td>
                                        <td>14.048387</td>
                                        <td>0.045055</td>
                                        <td>5.829851</td>
                                    </tr>
                                    <tr>
                                        <th>71</th>
                                        <td>(Palmade)</td>
                                        <td>(Pierre)</td>
                                        <td>0.041045</td>
                                        <td>1.000000</td>
                                        <td>22.333333</td>
                                        <td>0.039207</td>
                                        <td>inf</td>
                                    </tr>
                                    <tr>
                                        <th>70</th>
                                        <td>(Pierre)</td>
                                        <td>(Palmade)</td>
                                        <td>0.041045</td>
                                        <td>0.916667</td>
                                        <td>22.333333</td>
                                        <td>0.039207</td>
                                        <td>11.507463</td>
                                    </tr>
                                    <tr>
                                        <th>459</th>
                                        <td>(Palmade)</td>
                                        <td>(accident, Pierre)</td>
                                        <td>0.027985</td>
                                        <td>0.681818</td>
                                        <td>24.363636</td>
                                        <td>0.026836</td>
                                        <td>3.054904</td>
                                    </tr>
                                    <tr>
                                        <th>454</th>
                                        <td>(accident, Pierre)</td>
                                        <td>(Palmade)</td>
                                        <td>0.027985</td>
                                        <td>1.000000</td>
                                        <td>24.363636</td>
                                        <td>0.026836</td>
                                        <td>inf</td>
                                    </tr>
                                    <tr>
                                        <th>457</th>
                                        <td>(accident)</td>
                                        <td>(Pierre, Palmade)</td>
                                        <td>0.027985</td>
                                        <td>0.937500</td>
                                        <td>22.840909</td>
                                        <td>0.026760</td>
                                        <td>15.343284</td>
                                    </tr>
                                    <tr>
                                        <th>456</th>
                                        <td>(Pierre, Palmade)</td>
                                        <td>(accident)</td>
                                        <td>0.027985</td>
                                        <td>0.681818</td>
                                        <td>22.840909</td>
                                        <td>0.026760</td>
                                        <td>3.049041</td>
                                    </tr>
                                    <tr>
                                        <th>73</th>
                                        <td>(Palmade)</td>
                                        <td>(accident)</td>
                                        <td>0.027985</td>
                                        <td>0.681818</td>
                                        <td>22.840909</td>
                                        <td>0.026760</td>
                                        <td>3.049041</td>
                                    </tr>
                                    <tr>
                                        <th>72</th>
                                        <td>(accident)</td>
                                        <td>(Palmade)</td>
                                        <td>0.027985</td>
                                        <td>0.937500</td>
                                        <td>22.840909</td>
                                        <td>0.026760</td>
                                        <td>15.343284</td>
                                    </tr>
                                    <tr>
                                        <th>458</th>
                                        <td>(Pierre)</td>
                                        <td>(accident, Palmade)</td>
                                        <td>0.027985</td>
                                        <td>0.625000</td>
                                        <td>22.333333</td>
                                        <td>0.026732</td>
                                        <td>2.592040</td>
                                    </tr>
                                    <tr>
                                        <th>455</th>
                                        <td>(accident, Palmade)</td>
                                        <td>(Pierre)</td>
                                        <td>0.027985</td>
                                        <td>1.000000</td>
                                        <td>22.333333</td>
                                        <td>0.026732</td>
                                        <td>inf</td>
                                    </tr>
                                    <tr>
                                        <th>95</th>
                                        <td>(Pierre)</td>
                                        <td>(accident)</td>
                                        <td>0.027985</td>
                                        <td>0.625000</td>
                                        <td>20.937500</td>
                                        <td>0.026648</td>
                                        <td>2.587065</td>
                                    </tr>
                                    <tr>
                                        <th>94</th>
                                        <td>(accident)</td>
                                        <td>(Pierre)</td>
                                        <td>0.027985</td>
                                        <td>0.937500</td>
                                        <td>20.937500</td>
                                        <td>0.026648</td>
                                        <td>15.283582</td>
                                    </tr>
                                    <tr>
                                        <th>231</th>
                                        <td>(réforme)</td>
                                        <td>(retraite)</td>
                                        <td>0.018657</td>
                                        <td>1.000000</td>
                                        <td>26.800000</td>
                                        <td>0.017961</td>
                                        <td>inf</td>
                                    </tr>
                                </tbody>
                            </table>

                        </div>
                        <p class="mt-4">Pour le critére de pertinence je suis parti sur le leverage car c'est celui qui donnait les résultats les plus prometteurs. Afin de merge les combinaisons on peut supposer que dans l'ordre décroissant de la pertinence si (x, y) et (x,
                            z) ont
                            x en commun alors on associe les deux et on obtient (x , y ,z), en prenant bien soin d'indexer la
                            combinaison avec la meilleure pertinence des deux.</p>
                        </p>
                        <div class="box">
                            <pre>
                            <code class="language-python">      
    criterion='leverage'
    level=0.01
    trends = []

    for i in rules.index:
        rule = rules.loc[i]
        x = list(rule['antecedents'])
        y = list(rule['consequents'])
        terms = x + y
        same = True
        new_trend = terms
        delete_trends_ids = []
        for term in terms:
            for i, trend in enumerate(trends):
                if (term in trend):
                same = False
                    old_trend = new_trend
                    # old_trend -> new_terms + old_trend
                    new_trend = list(set(new_trend + list(trend)))
                    delete_trends_ids.append(i)
        if (same == True):
            trends.append((tuple(y + x)))
        else:
            trends = [x for i, x in enumerate(trends) if i not in delete_trends_ids]
            trends.insert(min(delete_trends_ids), tuple(new_trend))
                            </code>
                        </pre>
                        </div>
                        <p>Après itération du code ci-dessus on obtient les "topics" suivants :</p>
                        <div class="box">
                            <pre>
                                <code class="language-python">
# Accident de voiture de Pierre Palmade testé positif à la cocaïne (mort d'un bébé dans l'accident)
('accident','Palmade','Pierre','homme','avocat','sœur','victime','humoriste','affaire','famille')

# Guerre en Ukraine
('Kiev', 'Otan', 'guerre', 'Moldavie', 'Ukraine', 'bakhmout', 'Russie', 'char')

# Réforme des retraites, avec Aurélien Pradié (député LR) qui s'abstient contre l'avis de son parti
('Pradié','LR','médecin','âge','cotisation','SNCF','février','an','jeudi','Aurélien',
   'RATP','enfant','perturbation','libéral','majorité','carrière','grève','réforme','long','retraite')

# Nikki Haley qui candidate à la présidentielle américaine de 2024 (face à Donald Trump) 
('Nikki','républicain','Trump','américain','présidentielle','candidat','Haley','Donald')

# Séisme en Turquie-Syrie
('séisme', 'Turquie', 'Syrie')

# Match 8e de finale Ligue des Champions (PSG - Bayern)
('Bayern', 'PSG')

# Europe vote fin des voitures à moteur thermique pour 2035
('européen','thermique','moteur','pollution','air','automobile',
   'particule','France','vote','parlement','fin')

# Reste de cadavre d'une femme découpée au parc des Buttes-Chaumont
('Chaumont', 'butte', 'humain', 'femme', 'reste')

# Bruno Benard (Lyon) repousse l'interdiction du diesel à 2028
('Bruno', 'Bernard', 'Lyon', 'président', 'zfe')

# Légère baisse du taux de chômage
('taux', 'chômage')

# Stade de France privé d'évènement en 2024 pour préparer les JO
('Paris', 'jo')

# Olivier Dussopt traité d'assassin par un député LFI à l'assemblée
('Dussopt', 'LFI', 'député')

# Marseille se préocupe des opérateurs de trottinettes électriques
('Marseille', 'trottinette')

# Bansky dévoile une nouvelle oeuvre sur les violences conjugales pour la St Valentin
('Banksy', 'œuvre')

# Des ballons chinois sont partout dans le monde
('chinois', 'espion', 'ballon')

# C'est proche de la Saint-Valentin
('Saint-Valentin', 'conjugal')
                                </code>
                            </pre>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</body>

<script>
    $(document).ready(function () {
        $('.dataframe').DataTable({ responsive: true, order: [[6, 'desc']] });
    });
</script>
<style>
    html,
    body {
        min-height: 100%;
        height: 100%;
        min-width: 100%;
        margin: 0;
        padding: 0;
    }

    .fill {
        min-height: 100%;
        height: 100%;
    }

    a,
    a:hover {
        color: inherit;
        text-decoration: inherit;
    }

    .light {
        background-color: white;
        color: black;
    }

    .aside {
        width: 500px;
        background-color: #F9BE08;
        color: black;
        margin: 0;
        padding: 30px;
        height: 100%;
        position: fixed;
        z-index: 10;
    }

    .datatable,
    .dataTable {
        background-color: #3a3a3a !important;
    }

    .container-right {
        margin: 0;
        padding: 0;
        padding-left: 500px;
        width: auto;
        min-inline-size: 50%;
    }

    .text-label {
        font-family: 'GilroyBold';
    }

    #iframeNetwork {
        position: relative;
        width: 100%;
        height: 100vh;
        background-color: white;
    }

    .title {
        font-family: 'GilroyBold';
        font-size: 1.5rem;
    }

    pre.language-python {
        background: #080808 !important;
        padding: 0 20px !important;

    }

    .box {
        margin: 20px 0;
    }

    p {
        margin: 0;
        font-size: 1.2rem;
        text-align: justify;
    }

    .wave-container {
        position: relative;
        color: #FFF;
        text-align: center;
        overflow: hidden;
        padding: 0;
        margin: 0;
        margin-bottom: -50px;
    }

    @media screen and (max-width: 1400px) {
        .aside {
            position: relative;
            height: auto;
            width: 100%;
        }

        .container-right {
            padding: 0;
            width: 100%;
        }
    }
</style>

</html>